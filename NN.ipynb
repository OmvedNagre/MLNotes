{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec7b9d7e",
   "metadata": {},
   "source": [
    "A Perceptron is the simplest form of a neural network that makes decisions by combining inputs with weights and applying an activation function on weighted sum of inpuuts + bias . It is mainly used for binary classification problems. It forms the basic building block of many deep learning models.\n",
    "\n",
    "Weights determine how strongly each input contributes to the prediction.\n",
    "\n",
    "\n",
    "The bias is a constant value added to the weighted sum to shift the decision boundary\n",
    "it Shift the decision boundary left/right/up/down \n",
    "without it the dicision boundary must pass from origin if all the features or inputs are zero ,\n",
    "with the bias added , the perceptron can make decision even when all the inputs are zero which better fits the real world data \n",
    "\n",
    "mathematics -> weights tilt the decision boundary and bias shifts it.\n",
    "\n",
    "weighted_sum(Z) = sum(WiXi) + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d120f630",
   "metadata": {},
   "source": [
    "now , we have to apply activation function on the weighted sum which will add non - linearity to the output as you can see the z is just a linear output of maths and the decision boundary is just a straight line \n",
    "on a single perceptron even if we add a activation function it will still out put a linear decision boundary for non-lineary or solving complex patterns we need multiple perceptrons in multiple layers . \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6d25e",
   "metadata": {},
   "source": [
    " ## A neural network extends the perceptron by connecting many neurons across multiple layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de02e8",
   "metadata": {},
   "source": [
    "1. Input layer: Each neuron or node in this layer corresponds to an input feature, \n",
    "2.Hidden layers: it contains multiple perceptrons (neurons) \n",
    "3.OutPut layer : The output layer produces the final prediction, which may be binary, multi-class or a continuous value. If there are multiple outputs, the output layer will have a corresponding number of neurons\n",
    "Output activation depends on the task:\n",
    "Sigmoid: binary classification\n",
    "Softmax: multi-class classification\n",
    "Linear: regression\n",
    "Because of multiple layers and non-linear activations, neural networks can model complex, non-linear decision boundaries, while a single perceptron can only model a straight line. \n",
    "\n",
    "every node in one layer connects to every node in the next layer. As the data moves through the network each layer transforms it until the final output is generated in the output layer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8a3837",
   "metadata": {},
   "source": [
    "Once the network generates an output the next step is to calculate the loss using a loss function. In supervised learning this compares the predicted output to the actual label.\n",
    "\n",
    "For a classification problem the commonly used binary cross-entropy loss function\n",
    "For regression problems the mean squared error (MSE) is often used . \n",
    "\n",
    "this all from input layer to weighted sum to activation function to output is forward propogation .  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986da99a",
   "metadata": {},
   "source": [
    "## Training a perceptron means finding suitable weights wi and bias b such that most training points are correctly classified. This is achieved through backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68334e4",
   "metadata": {},
   "source": [
    "backpropogation : its goal is to minimizing the difference between predicted and actual outputs. It works by propagating errors backward through the network, using the chain rule of calculus to compute gradients and then iteratively updating the weights and biases. Combined with optimization techniques like gradient descent, backpropagation enables the model to reduce loss across epochs and effectively learn complex patterns from data.\n",
    "\n",
    "These gradients indicate how much each weight and bias should be adjusted to minimize the error in the next iteration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c84959",
   "metadata": {},
   "source": [
    "now we will implement a MLP on MNIST data to predict the handwritten number . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8da24ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (2.4.2)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (3.0.0)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (1.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/anantjain/Library/Python/3.14/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anantjain/Library/Python/3.14/lib/python/site-packages (from matplotlib) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anantjain/Library/Python/3.14/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas matplotlib seaborn scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b27df95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (25.3)\n",
      "Collecting pip\n",
      "  Downloading pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-26.0.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.3\n",
      "    Uninstalling pip-25.3:\n",
      "      Successfully uninstalled pip-25.3\n",
      "\u001b[33m  WARNING: The scripts pip, pip3 and pip3.14 are installed in '/Library/Frameworks/Python.framework/Versions/3.14/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pip-26.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f3fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5078dde",
   "metadata": {},
   "source": [
    "Loading MNIST Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "690411a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c114eeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)  # (60000, 28, 28)\n",
    "print(y_train.shape)  # (60000,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b07f0",
   "metadata": {},
   "source": [
    "Flatten the images (CRUCIAL)\n",
    "\n",
    "MLP expects 1D input, not 2D images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test  = X_test.reshape(10000, 784)\n",
    "#Each pixel becomes one input neuron\n",
    "#Total input neurons = 784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a8fa5",
   "metadata": {},
   "source": [
    "Normalize the data (VERY IMPORTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4960fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test  = X_test / 255.0\n",
    "# Neural networks learn better with values in 0–1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70def5",
   "metadata": {},
   "source": [
    "One-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adbc3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 10)\n",
    "y_test  = to_categorical(y_test, 10)\n",
    "#We use softmax activation in the output layer, so we need to convert labels to one-hot encoding\n",
    "# Model outputs probabilities for all classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf08ca6",
   "metadata": {},
   "source": [
    "Build the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "665a352f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/keras/src/layers/core/dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(128,\n",
    "    input_shape=(784,),\n",
    "    activation='relu'),  #Input layer with 784 neurons, hidden layer with 128 neurons and ReLU activation adds non-linearity\n",
    "    Dense(64, activation='relu'), \n",
    "    Dense(10, activation='softmax') #Output layer with 10 neurons (one for each class) and softmax activation to get probabilities for each class\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa7b55",
   "metadata": {},
   "source": [
    "Compiling the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a17fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Adam → smart gradient descent\n",
    "# Categorical crossentropy → multi-class loss\n",
    "# Accuracy → evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5852e049",
   "metadata": {},
   "source": [
    "Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84342641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8895 - loss: 0.3835 - val_accuracy: 0.9473 - val_loss: 0.1797\n",
      "Epoch 2/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step - accuracy: 0.9551 - loss: 0.1536 - val_accuracy: 0.9623 - val_loss: 0.1315\n",
      "Epoch 3/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - accuracy: 0.9681 - loss: 0.1079 - val_accuracy: 0.9655 - val_loss: 0.1182\n",
      "Epoch 4/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - accuracy: 0.9756 - loss: 0.0819 - val_accuracy: 0.9672 - val_loss: 0.1056\n",
      "Epoch 5/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - accuracy: 0.9819 - loss: 0.0634 - val_accuracy: 0.9717 - val_loss: 0.0966\n",
      "Epoch 6/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - accuracy: 0.9844 - loss: 0.0512 - val_accuracy: 0.9748 - val_loss: 0.0884\n",
      "Epoch 7/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.9875 - loss: 0.0409 - val_accuracy: 0.9702 - val_loss: 0.1006\n",
      "Epoch 8/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - accuracy: 0.9899 - loss: 0.0337 - val_accuracy: 0.9753 - val_loss: 0.0886\n",
      "Epoch 9/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - accuracy: 0.9923 - loss: 0.0255 - val_accuracy: 0.9741 - val_loss: 0.0931\n",
      "Epoch 10/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - accuracy: 0.9927 - loss: 0.0228 - val_accuracy: 0.9746 - val_loss: 0.0951\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea578f6",
   "metadata": {},
   "source": [
    "Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ca72769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - accuracy: 0.9776 - loss: 0.0818\n",
      "Test Accuracy: 0.9775999784469604\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2d6973",
   "metadata": {},
   "source": [
    "Predict a digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be6a4fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted digit: 7\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test[0].reshape(1, 784))\n",
    "print(\"Predicted digit:\", np.argmax(prediction))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
